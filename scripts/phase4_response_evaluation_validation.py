#!/usr/bin/env python3
"""
Phase 4.3: Response Quality Evaluation Validation
Test relevance scoring and response assessment with real data
"""

import sys
import os
import json
import time
import asyncio
from pathlib import Path
from typing import List, Dict, Any, Tuple
import numpy as np
from datetime import datetime
import re

# Add data-pipeline services to path
data_pipeline_src = Path(__file__).parent.parent / "services" / "data-pipeline" / "src"
sys.path.insert(0, str(data_pipeline_src))

try:
    from embeddings.embedding_service import EmbeddingService
    from vector.chroma_service import ChromaService
    from search.semantic_search_service import SemanticSearchService
    from processing.content_processor import ContentProcessor
    from search.reranker import ReRanker
    from search.topic_detector import TopicDetector
    from search.smart_document_filter import SmartDocumentFilter
    from processing.advanced_content_processor import AdvancedContentProcessor
except ImportError as e:
    print(f"‚ùå Import error: {e}")
    print("Make sure data-pipeline services are properly set up")
    sys.exit(1)

class ResponseQualityValidator:
    """Validate response quality evaluation with real data"""
    
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "phase": "4.3 - Response Quality Evaluation Validation",
            "status": "running",
            "tests": {},
            "summary": {}
        }
        
        # Initialize services
        self.embedding_service = EmbeddingService()
        self.chroma_service = ChromaService()
        self.search_service = SemanticSearchService(self.chroma_service, self.embedding_service)
        self.content_processor = ContentProcessor()
        self.reranker = ReRanker()
        self.topic_detector = TopicDetector()
        self.smart_filter = SmartDocumentFilter()
        self.advanced_processor = AdvancedContentProcessor()
        
        # Vault data path
        self.vault_path = Path("D:/Nomade Milionario")
        
        # Test cases for response quality evaluation
        self.test_cases = [
            {
                "query": "What are the main philosophical currents of logic and mathematics?",
                "good_response": "Based on the documents, the main philosophical currents are: 1) Logicism (Frege, Russell) - mathematics is reducible to logic, 2) Formalism (Hilbert) - mathematics is a game of symbols, 3) Intuitionism (Brouwer) - mathematics is mental construction. Sources: LOGICA-INDICE.md, filosofia.md",
                "bad_response": "I don't have specific information about philosophical currents. Please check the documents for more details.",
                "expected_keywords": ["logicism", "formalism", "intuitionism", "philosophy", "mathematics", "logic"]
            },
            {
                "query": "How does Scrapy handle web scraping?",
                "good_response": "Scrapy is a Python framework for web scraping that uses spiders to crawl websites. It handles requests, responses, and data extraction through selectors. Key features include built-in support for handling cookies, sessions, and anti-bot measures.",
                "bad_response": "Scrapy is a tool for web development. It helps with building websites and applications.",
                "expected_keywords": ["scrapy", "web scraping", "spiders", "python", "crawl", "selectors"]
            },
            {
                "query": "What is the PQLP reading technique?",
                "good_response": "PQLP (Preview, Question, Learn, Practice) is a reading technique that involves previewing material, generating questions, learning actively, and practicing retention. It's designed to improve reading comprehension and speed.",
                "bad_response": "PQLP is a general reading method. It helps with understanding text.",
                "expected_keywords": ["pqlp", "preview", "question", "learn", "practice", "reading", "technique"]
            }
        ]
    
    def calculate_relevance_score(self, query: str, response: str, expected_keywords: List[str]) -> float:
        """Calculate relevance score based on keyword overlap and semantic similarity"""
        # Keyword overlap score
        query_words = set(query.lower().split())
        response_words = set(response.lower().split())
        expected_words = set(expected_keywords)
        
        # Calculate overlaps
        query_response_overlap = len(query_words & response_words) / len(query_words) if query_words else 0
        response_expected_overlap = len(response_words & expected_words) / len(expected_words) if expected_words else 0
        
        # Combine scores
        relevance_score = (0.6 * query_response_overlap) + (0.4 * response_expected_overlap)
        return min(1.0, max(0.0, relevance_score))
    
    def calculate_completeness_score(self, response: str, expected_keywords: List[str]) -> float:
        """Calculate completeness score based on coverage of expected keywords"""
        response_lower = response.lower()
        covered_keywords = sum(1 for keyword in expected_keywords if keyword in response_lower)
        completeness_score = covered_keywords / len(expected_keywords) if expected_keywords else 0
        return min(1.0, max(0.0, completeness_score))
    
    def calculate_coherence_score(self, response: str) -> float:
        """Calculate coherence score based on response structure and flow"""
        # Check for proper sentence structure
        sentences = re.split(r'[.!?]+', response)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return 0.0
        
        # Check for proper capitalization and punctuation
        proper_sentences = 0
        for sentence in sentences:
            if sentence and sentence[0].isupper() and (sentence.endswith('.') or sentence.endswith('!') or sentence.endswith('?')):
                proper_sentences += 1
        
        structure_score = proper_sentences / len(sentences)
        
        # Check for logical flow indicators
        flow_indicators = ['first', 'second', 'third', '1)', '2)', '3)', 'based on', 'according to', 'however', 'therefore', 'furthermore']
        flow_count = sum(1 for indicator in flow_indicators if indicator in response.lower())
        flow_score = min(1.0, flow_count / 3)  # Normalize to 0-1
        
        # Combine scores
        coherence_score = (0.7 * structure_score) + (0.3 * flow_score)
        return min(1.0, max(0.0, coherence_score))
    
    def calculate_citation_score(self, response: str, retrieved_docs: List[Dict]) -> float:
        """Calculate citation score based on proper source attribution"""
        if not retrieved_docs:
            return 0.0
        
        # Check for source mentions
        source_mentions = 0
        for doc in retrieved_docs:
            doc_name = doc.get('path', '').replace('.md', '')
            if doc_name.lower() in response.lower():
                source_mentions += 1
        
        citation_score = source_mentions / len(retrieved_docs)
        return min(1.0, max(0.0, citation_score))
    
    def calculate_overall_quality_score(self, query: str, response: str, retrieved_docs: List[Dict], expected_keywords: List[str]) -> Dict[str, float]:
        """Calculate overall quality score with all metrics"""
        relevance_score = self.calculate_relevance_score(query, response, expected_keywords)
        completeness_score = self.calculate_completeness_score(response, expected_keywords)
        coherence_score = self.calculate_coherence_score(response)
        citation_score = self.calculate_citation_score(response, retrieved_docs)
        
        # Overall score (weighted average)
        overall_score = (0.3 * relevance_score + 0.3 * completeness_score + 0.2 * coherence_score + 0.2 * citation_score)
        
        return {
            "relevance_score": relevance_score,
            "completeness_score": completeness_score,
            "coherence_score": coherence_score,
            "citation_score": citation_score,
            "overall_quality_score": overall_score
        }
    
    def test_response_quality_metrics(self) -> Dict[str, Any]:
        """Test response quality metrics calculation"""
        print("üìä Testing Response Quality Metrics...")
        
        test_results = {
            "test_name": "Response Quality Metrics",
            "status": "running",
            "metrics_tests": {},
            "overall_success": True
        }
        
        try:
            # Test 1: Individual metric calculations
            print("  üîç Testing individual metric calculations...")
            
            test_case = self.test_cases[0]
            query = test_case["query"]
            good_response = test_case["good_response"]
            bad_response = test_case["bad_response"]
            expected_keywords = test_case["expected_keywords"]
            
            # Test good response
            good_scores = self.calculate_overall_quality_score(
                query, good_response, [], expected_keywords
            )
            
            # Test bad response
            bad_scores = self.calculate_overall_quality_score(
                query, bad_response, [], expected_keywords
            )
            
            # Validate that good response scores higher
            assert good_scores["overall_quality_score"] > bad_scores["overall_quality_score"], "Good response should score higher than bad response"
            
            test_results["metrics_tests"]["individual_metrics"] = {
                "status": "passed",
                "good_scores": good_scores,
                "bad_scores": bad_scores,
                "message": "Individual metrics calculation working"
            }
            
            print("    ‚úÖ Individual metrics calculation working")
            
            # Test 2: Edge cases
            print("  üîç Testing edge cases...")
            
            # Empty response
            empty_scores = self.calculate_overall_quality_score("test", "", [], [])
            assert empty_scores["overall_quality_score"] == 0.0, "Empty response should have 0 quality score"
            
            # Very short response
            short_scores = self.calculate_overall_quality_score("test", "yes", [], [])
            assert short_scores["overall_quality_score"] < 0.5, "Very short response should have low quality score"
            
            test_results["metrics_tests"]["edge_cases"] = {
                "status": "passed",
                "empty_scores": empty_scores,
                "short_scores": short_scores,
                "message": "Edge cases handled correctly"
            }
            
            print("    ‚úÖ Edge cases handled correctly")
            
            # Test 3: Citation scoring
            print("  üìö Testing citation scoring...")
            
            mock_docs = [
                {"path": "LOGICA-INDICE.md", "similarity": 0.89, "content": "Logic content"},
                {"path": "filosofia.md", "similarity": 0.87, "content": "Philosophy content"}
            ]
            
            response_with_citations = "Based on LOGICA-INDICE.md and filosofia.md, the main currents are..."
            response_without_citations = "The main currents are logicism, formalism, and intuitionism."
            
            citation_scores_with = self.calculate_citation_score(response_with_citations, mock_docs)
            citation_scores_without = self.calculate_citation_score(response_without_citations, mock_docs)
            
            assert citation_scores_with > citation_scores_without, "Response with citations should score higher"
            
            test_results["metrics_tests"]["citation_scoring"] = {
                "status": "passed",
                "with_citations": citation_scores_with,
                "without_citations": citation_scores_without,
                "message": "Citation scoring working correctly"
            }
            
            print("    ‚úÖ Citation scoring working correctly")
            
            test_results["status"] = "passed"
            print("  ‚úÖ Response quality metrics test passed")
            
        except Exception as e:
            test_results["status"] = "failed"
            test_results["error"] = str(e)
            test_results["overall_success"] = False
            print(f"  ‚ùå Response quality metrics test failed: {e}")
        
        return test_results
    
    def test_real_data_evaluation(self) -> Dict[str, Any]:
        """Test response quality evaluation with real data"""
        print("üîó Testing Real Data Evaluation...")
        
        test_results = {
            "test_name": "Real Data Evaluation",
            "status": "running",
            "real_data_tests": {},
            "overall_success": True
        }
        
        try:
            # Test 1: Real vault content evaluation
            print("  üìÅ Testing real vault content evaluation...")
            
            if self.vault_path.exists():
                # Load real vault content
                vault_content = self._load_vault_content()
                
                if vault_content:
                    # Test with real content
                    real_query = "What are the main philosophical currents of logic and mathematics?"
                    real_results = self.search_service.search(real_query, list(vault_content.values()), top_k=5)
                    
                    # Generate response
                    real_response = f"Based on {len(real_results)} documents, here are the main philosophical currents..."
                    
                    # Calculate quality scores
                    real_scores = self.calculate_overall_quality_score(
                        real_query, real_response, real_results, ["philosophy", "mathematics", "logic"]
                    )
                    
                    test_results["real_data_tests"]["vault_content"] = {
                        "status": "passed",
                        "scores": real_scores,
                        "results_count": len(real_results),
                        "message": "Real vault content evaluation working"
                    }
                    
                    print("    ‚úÖ Real vault content evaluation working")
                else:
                    test_results["real_data_tests"]["vault_content"] = {
                        "status": "skipped",
                        "message": "No vault content found"
                    }
                    print("    ‚ö†Ô∏è No vault content found")
            else:
                test_results["real_data_tests"]["vault_content"] = {
                    "status": "skipped",
                    "message": "Vault path not found"
                }
                print("    ‚ö†Ô∏è Vault path not found")
            
            # Test 2: Performance with evaluation
            print("  ‚ö° Testing performance with evaluation...")
            
            start_time = time.time()
            
            # Test all test cases
            evaluation_results = []
            for test_case in self.test_cases:
                scores = self.calculate_overall_quality_score(
                    test_case["query"],
                    test_case["good_response"],
                    [],
                    test_case["expected_keywords"]
                )
                evaluation_results.append(scores)
            
            end_time = time.time()
            total_time = end_time - start_time
            
            test_results["real_data_tests"]["performance"] = {
                "status": "passed",
                "total_time": total_time,
                "average_time": total_time / len(self.test_cases),
                "evaluation_results": evaluation_results,
                "message": f"Performance test completed in {total_time:.3f}s"
            }
            
            print(f"    ‚úÖ Performance test: {total_time:.3f}s")
            
            test_results["status"] = "passed"
            print("  ‚úÖ Real data evaluation test passed")
            
        except Exception as e:
            test_results["status"] = "failed"
            test_results["error"] = str(e)
            test_results["overall_success"] = False
            print(f"  ‚ùå Real data evaluation test failed: {e}")
        
        return test_results
    
    def test_quality_thresholds(self) -> Dict[str, Any]:
        """Test quality thresholds and classification"""
        print("üéØ Testing Quality Thresholds...")
        
        test_results = {
            "test_name": "Quality Thresholds",
            "status": "running",
            "threshold_tests": {},
            "overall_success": True
        }
        
        try:
            # Test 1: Quality classification
            print("  üìä Testing quality classification...")
            
            def classify_quality(overall_score: float) -> str:
                if overall_score >= 0.8:
                    return "excellent"
                elif overall_score >= 0.6:
                    return "good"
                elif overall_score >= 0.4:
                    return "fair"
                else:
                    return "poor"
            
            # Test different quality levels
            test_scores = [0.9, 0.7, 0.5, 0.3, 0.1]
            classifications = [classify_quality(score) for score in test_scores]
            
            expected_classifications = ["excellent", "good", "fair", "poor", "poor"]
            assert classifications == expected_classifications, "Quality classification should match expected"
            
            test_results["threshold_tests"]["classification"] = {
                "status": "passed",
                "test_scores": test_scores,
                "classifications": classifications,
                "message": "Quality classification working"
            }
            
            print("    ‚úÖ Quality classification working")
            
            # Test 2: Threshold validation
            print("  üîç Testing threshold validation...")
            
            # Test with real test cases
            threshold_results = []
            for test_case in self.test_cases:
                good_scores = self.calculate_overall_quality_score(
                    test_case["query"],
                    test_case["good_response"],
                    [],
                    test_case["expected_keywords"]
                )
                bad_scores = self.calculate_overall_quality_score(
                    test_case["query"],
                    test_case["bad_response"],
                    [],
                    test_case["expected_keywords"]
                )
                
                threshold_results.append({
                    "query": test_case["query"],
                    "good_classification": classify_quality(good_scores["overall_quality_score"]),
                    "bad_classification": classify_quality(bad_scores["overall_quality_score"]),
                    "good_score": good_scores["overall_quality_score"],
                    "bad_score": bad_scores["overall_quality_score"]
                })
            
            test_results["threshold_tests"]["validation"] = {
                "status": "passed",
                "threshold_results": threshold_results,
                "message": "Threshold validation working"
            }
            
            print("    ‚úÖ Threshold validation working")
            
            test_results["status"] = "passed"
            print("  ‚úÖ Quality thresholds test passed")
            
        except Exception as e:
            test_results["status"] = "failed"
            test_results["error"] = str(e)
            test_results["overall_success"] = False
            print(f"  ‚ùå Quality thresholds test failed: {e}")
        
        return test_results
    
    def _load_vault_content(self) -> Dict[str, Dict]:
        """Load vault content for testing"""
        vault_content = {}
        
        try:
            # Load a few sample files for testing
            sample_files = [
                "LOGICA-INDICE.md",
                "Hiper-Leitura.md",
                "scrapy.md"
            ]
            
            for filename in sample_files:
                file_path = self.vault_path / filename
                if file_path.exists():
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # Process content
                    processed_content = self.content_processor.process_file(file_path)
                    vault_content[filename] = processed_content
            
            return vault_content
            
        except Exception as e:
            print(f"    ‚ö†Ô∏è Error loading vault content: {e}")
            return {}
    
    def run_validation(self) -> Dict[str, Any]:
        """Run complete Phase 4.3 validation"""
        print("üöÄ Starting Phase 4.3: Response Quality Evaluation Validation")
        print("=" * 60)
        
        # Test 1: Response quality metrics
        metrics_test = self.test_response_quality_metrics()
        self.results["tests"]["response_quality_metrics"] = metrics_test
        
        # Test 2: Real data evaluation
        real_data_test = self.test_real_data_evaluation()
        self.results["tests"]["real_data_evaluation"] = real_data_test
        
        # Test 3: Quality thresholds
        thresholds_test = self.test_quality_thresholds()
        self.results["tests"]["quality_thresholds"] = thresholds_test
        
        # Calculate overall success
        overall_success = all(
            test.get("overall_success", False) 
            for test in self.results["tests"].values()
        )
        
        self.results["status"] = "completed" if overall_success else "failed"
        self.results["summary"] = {
            "overall_success": overall_success,
            "tests_completed": len(self.results["tests"]),
            "successful_tests": sum(1 for test in self.results["tests"].values() if test.get("overall_success", False)),
            "failed_tests": sum(1 for test in self.results["tests"].values() if not test.get("overall_success", False))
        }
        
        print("\n" + "=" * 60)
        print("üìä Phase 4.3 Validation Summary:")
        print(f"Overall Success: {'‚úÖ YES' if overall_success else '‚ùå NO'}")
        print(f"Tests Completed: {self.results['summary']['tests_completed']}")
        print(f"Successful Tests: {self.results['summary']['successful_tests']}")
        print(f"Failed Tests: {self.results['summary']['failed_tests']}")
        
        return self.results

def main():
    """Main validation function"""
    validator = ResponseQualityValidator()
    results = validator.run_validation()
    
    # Save results
    output_file = "phase4_response_evaluation_validation_results.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ Results saved to: {output_file}")
    
    return results

if __name__ == "__main__":
    main()
