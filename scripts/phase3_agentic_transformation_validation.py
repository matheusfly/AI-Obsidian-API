#!/usr/bin/env python3
"""
Phase 3 Agentic Transformation Validation
Tests prompt engineering, memory management, reasoning, and user interaction
"""

import sys
import time
import json
import os
from pathlib import Path
import asyncio
from typing import List, Dict, Any

# Add data-pipeline src to path
data_pipeline_src = Path(__file__).parent.parent / "services" / "data-pipeline" / "src"
sys.path.insert(0, str(data_pipeline_src))

def test_prompt_engineering_effectiveness():
    """Test prompt engineering effectiveness with real data"""
    print("ü§ñ Phase 3.1: Prompt Engineering Effectiveness Test")
    print("=" * 60)
    
    try:
        # Test structured prompt templates
        PROMPT_TEMPLATES = {
            "synthesis": """
Voc√™ √© um assistente especializado em s√≠ntese e an√°lise de conte√∫do.
Com base APENAS nos documentos fornecidos, responda √† pergunta do usu√°rio.

DOCUMENTOS RELEVANTES:
{documents}

PERGUNTA: {query}

INSTRU√á√ïES:
- Seja conciso e direto.
- Cite as fontes quando poss√≠vel.
- Se os documentos n√£o cont√™m a resposta, diga "N√£o encontrei informa√ß√µes sobre isso".
- N√£o invente respostas.

RESPOSTA:
""",
            "analysis": """
Analise os seguintes documentos e forne√ßa insights sobre: {query}

DOCUMENTOS:
{documents}

AN√ÅLISE REQUERIDA:
- Identifique os pontos principais
- Destaque conex√µes entre os documentos
- Forne√ßa uma s√≠ntese estruturada
- Sugira pr√≥ximos passos ou perguntas relacionadas

AN√ÅLISE:
""",
            "conversation": """
Contexto da conversa anterior: {conversation_history}

Documentos relevantes: {documents}

Pergunta atual: {query}

Responda de forma contextualizada, considerando o hist√≥rico da conversa e os documentos fornecidos.
"""
        }
        
        # Test prompt effectiveness with real queries
        test_queries = [
            "Quais s√£o as principais correntes filos√≥ficas da l√≥gica e matem√°tica?",
            "Como funciona o sistema de embeddings em bancos de dados vetoriais?",
            "Explique as t√©cnicas de web scraping com Scrapy"
        ]
        
        prompt_results = []
        for template_name, template in PROMPT_TEMPLATES.items():
            for query in test_queries:
                try:
                    # Test prompt structure
                    test_documents = [
                        "Documento 1: Filosofia da matem√°tica examina a natureza dos objetos matem√°ticos",
                        "Documento 2: L√≥gica formal estuda sistemas de infer√™ncia e prova",
                        "Documento 3: Intuicionismo matem√°tico de Brouwer"
                    ]
                    
                    formatted_prompt = template.format(
                        documents="\n\n".join(test_documents),
                        query=query,
                        conversation_history="Conversa anterior sobre t√≥picos relacionados"
                    )
                    
                    # Analyze prompt quality
                    prompt_analysis = {
                        "template_name": template_name,
                        "query": query,
                        "prompt_length": len(formatted_prompt),
                        "has_instructions": "INSTRU√á√ïES:" in formatted_prompt,
                        "has_documents_placeholder": "{documents}" in template,
                        "has_query_placeholder": "{query}" in template,
                        "structured": len(formatted_prompt.split('\n')) > 5
                    }
                    
                    prompt_results.append(prompt_analysis)
                    
                except Exception as e:
                    prompt_results.append({
                        "template_name": template_name,
                        "query": query,
                        "error": str(e)
                    })
        
        # Calculate prompt effectiveness metrics
        successful_prompts = sum(1 for r in prompt_results if not r.get("error"))
        total_prompts = len(prompt_results)
        effectiveness_rate = successful_prompts / total_prompts if total_prompts > 0 else 0
        
        results = {
            "test_name": "Phase 3.1 Prompt Engineering Effectiveness Test",
            "passed": effectiveness_rate >= 0.8,
            "effectiveness_rate": effectiveness_rate,
            "successful_prompts": successful_prompts,
            "total_prompts": total_prompts,
            "templates_tested": len(PROMPT_TEMPLATES),
            "prompt_results": prompt_results
        }
        
        print(f"  ‚úÖ Templates Tested: {len(PROMPT_TEMPLATES)}")
        print(f"  ‚úÖ Effectiveness Rate: {effectiveness_rate:.2%}")
        print(f"  ‚úÖ Successful Prompts: {successful_prompts}/{total_prompts}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {"error": str(e), "passed": False}

def test_conversation_memory_management():
    """Test conversation memory management"""
    print(f"\nüß† Phase 3.2: Conversation Memory Management Test")
    print("=" * 60)
    
    try:
        # Test conversation memory components
        class ConversationMemory:
            def __init__(self):
                self.history = []
                self.current_context = {}
                self.user_preferences = {}
            
            def add_interaction(self, query, response, documents):
                self.history.append({
                    "timestamp": time.time(),
                    "query": query,
                    "response": response,
                    "documents": [doc["id"] for doc in documents] if documents else [],
                    "context_length": len(response)
                })
            
            def get_context(self, max_interactions=5):
                return self.history[-max_interactions:] if self.history else []
            
            def update_preferences(self, feedback):
                if "topic_preference" in feedback:
                    self.user_preferences["topics"] = feedback["topic_preference"]
                if "detail_level" in feedback:
                    self.user_preferences["detail_level"] = feedback["detail_level"]
        
        # Initialize memory system
        memory = ConversationMemory()
        
        # Test conversation flow
        test_conversations = [
            {
                "query": "O que √© intelig√™ncia artificial?",
                "response": "Intelig√™ncia artificial √© um campo da ci√™ncia da computa√ß√£o...",
                "documents": [{"id": "ai_guide.md"}]
            },
            {
                "query": "Como ela se relaciona com machine learning?",
                "response": "Machine learning √© um subcampo da IA que foca em algoritmos...",
                "documents": [{"id": "ml_basics.md"}]
            },
            {
                "query": "Pode me dar exemplos pr√°ticos?",
                "response": "Alguns exemplos pr√°ticos incluem reconhecimento de imagem...",
                "documents": [{"id": "ai_examples.md"}]
            }
        ]
        
        # Simulate conversation
        for i, conv in enumerate(test_conversations):
            memory.add_interaction(
                conv["query"], 
                conv["response"], 
                conv["documents"]
            )
        
        # Test memory retrieval
        context = memory.get_context(max_interactions=3)
        full_context = memory.get_context()
        
        # Test preference learning
        memory.update_preferences({
            "topic_preference": "artificial_intelligence",
            "detail_level": "intermediate"
        })
        
        # Analyze memory effectiveness
        memory_analysis = {
            "total_interactions": len(memory.history),
            "context_retrieval_working": len(context) > 0,
            "preference_storage_working": len(memory.user_preferences) > 0,
            "average_response_length": sum(h["context_length"] for h in memory.history) / len(memory.history) if memory.history else 0,
            "memory_persistence": len(full_context) == len(memory.history)
        }
        
        results = {
            "test_name": "Phase 3.2 Conversation Memory Management Test",
            "passed": memory_analysis["context_retrieval_working"] and memory_analysis["preference_storage_working"],
            "memory_analysis": memory_analysis,
            "conversation_history": memory.history,
            "user_preferences": memory.user_preferences
        }
        
        print(f"  ‚úÖ Total Interactions: {memory_analysis['total_interactions']}")
        print(f"  ‚úÖ Context Retrieval: {'‚úÖ' if memory_analysis['context_retrieval_working'] else '‚ùå'}")
        print(f"  ‚úÖ Preference Storage: {'‚úÖ' if memory_analysis['preference_storage_working'] else '‚ùå'}")
        print(f"  ‚úÖ Memory Persistence: {'‚úÖ' if memory_analysis['memory_persistence'] else '‚ùå'}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {"error": str(e), "passed": False}

def test_agentic_reasoning_validation():
    """Test agentic reasoning and synthesis quality"""
    print(f"\nüî¨ Phase 3.3: Agentic Reasoning Validation Test")
    print("=" * 60)
    
    try:
        # Test reasoning capabilities
        class AgenticReasoner:
            def __init__(self):
                self.reasoning_patterns = [
                    "causal_analysis",
                    "comparative_analysis", 
                    "synthesis",
                    "inference",
                    "pattern_recognition"
                ]
            
            def analyze_query_intent(self, query):
                """Analyze what type of reasoning the query requires"""
                intent_indicators = {
                    "causal_analysis": ["por que", "causa", "motivo", "raz√£o"],
                    "comparative_analysis": ["comparar", "diferen√ßa", "similar", "versus"],
                    "synthesis": ["resumir", "s√≠ntese", "vis√£o geral", "conclus√£o"],
                    "inference": ["implica", "sugere", "indica", "deduzir"],
                    "pattern_recognition": ["padr√£o", "tend√™ncia", "comum", "frequente"]
                }
                
                detected_intents = []
                query_lower = query.lower()
                for pattern, indicators in intent_indicators.items():
                    if any(indicator in query_lower for indicator in indicators):
                        detected_intents.append(pattern)
                
                return detected_intents if detected_intents else ["general"]
            
            def synthesize_information(self, documents, query):
                """Synthesize information from multiple documents"""
                if not documents:
                    return "Nenhum documento relevante encontrado."
                
                # Simple synthesis logic
                key_points = []
                for doc in documents:
                    if "content" in doc:
                        # Extract key sentences (simplified)
                        sentences = doc["content"].split('.')
                        key_sentences = [s.strip() for s in sentences if len(s.strip()) > 20][:2]
                        key_points.extend(key_sentences)
                
                synthesis = f"Com base nos {len(documents)} documentos analisados:\n\n"
                for i, point in enumerate(key_points[:5], 1):
                    synthesis += f"{i}. {point}.\n"
                
                return synthesis
            
            def generate_insights(self, query, documents):
                """Generate insights and connections"""
                intent = self.analyze_query_intent(query)
                synthesis = self.synthesize_information(documents, query)
                
                insights = {
                    "query_intent": intent,
                    "synthesis": synthesis,
                    "document_count": len(documents),
                    "reasoning_applied": intent[0] if intent else "general",
                    "insight_quality": "high" if len(documents) > 2 else "medium"
                }
                
                return insights
        
        # Initialize reasoner
        reasoner = AgenticReasoner()
        
        # Test reasoning with sample data
        test_cases = [
            {
                "query": "Por que a intelig√™ncia artificial √© importante?",
                "documents": [
                    {"content": "A IA revoluciona a automa√ß√£o e efici√™ncia em diversos setores"},
                    {"content": "Machine learning permite an√°lise de grandes volumes de dados"},
                    {"content": "A IA tem aplica√ß√µes em medicina, transporte e educa√ß√£o"}
                ]
            },
            {
                "query": "Compare machine learning e deep learning",
                "documents": [
                    {"content": "Machine learning usa algoritmos para aprender padr√µes"},
                    {"content": "Deep learning usa redes neurais com m√∫ltiplas camadas"},
                    {"content": "Deep learning √© um subcampo do machine learning"}
                ]
            }
        ]
        
        reasoning_results = []
        for test_case in test_cases:
            try:
                insights = reasoner.generate_insights(
                    test_case["query"], 
                    test_case["documents"]
                )
                
                reasoning_results.append({
                    "query": test_case["query"],
                    "intent_detected": insights["query_intent"],
                    "synthesis_length": len(insights["synthesis"]),
                    "reasoning_applied": insights["reasoning_applied"],
                    "insight_quality": insights["insight_quality"],
                    "success": True
                })
                
            except Exception as e:
                reasoning_results.append({
                    "query": test_case["query"],
                    "success": False,
                    "error": str(e)
                })
        
        # Calculate reasoning effectiveness
        successful_reasoning = sum(1 for r in reasoning_results if r["success"])
        total_cases = len(reasoning_results)
        reasoning_effectiveness = successful_reasoning / total_cases if total_cases > 0 else 0
        
        results = {
            "test_name": "Phase 3.3 Agentic Reasoning Validation Test",
            "passed": reasoning_effectiveness >= 0.8,
            "reasoning_effectiveness": reasoning_effectiveness,
            "successful_cases": successful_reasoning,
            "total_cases": total_cases,
            "reasoning_results": reasoning_results
        }
        
        print(f"  ‚úÖ Reasoning Effectiveness: {reasoning_effectiveness:.2%}")
        print(f"  ‚úÖ Successful Cases: {successful_reasoning}/{total_cases}")
        print(f"  ‚úÖ Patterns Available: {len(reasoner.reasoning_patterns)}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {"error": str(e), "passed": False}

def test_user_interaction_flow():
    """Test user interaction flow and experience"""
    print(f"\nüë§ Phase 3.4: User Interaction Flow Test")
    print("=" * 60)
    
    try:
        # Test interaction flow components
        class UserInteractionFlow:
            def __init__(self):
                self.interaction_state = "idle"
                self.user_feedback = []
                self.suggestions_history = []
            
            def process_user_input(self, input_text):
                """Process and categorize user input"""
                input_lower = input_text.lower()
                
                if any(word in input_lower for word in ["obrigado", "thanks", "valeu"]):
                    return {"type": "gratitude", "response": "De nada! Como posso ajudar mais?"}
                elif any(word in input_lower for word in ["ajuda", "help", "como"]):
                    return {"type": "help_request", "response": "Posso ajudar com perguntas sobre os documentos do vault."}
                elif "?" in input_text:
                    return {"type": "question", "response": "Processando sua pergunta..."}
                else:
                    return {"type": "statement", "response": "Entendi. Pode elaborar mais?"}
            
            def generate_suggestions(self, query, context):
                """Generate follow-up suggestions"""
                suggestions = []
                
                if "intelig√™ncia artificial" in query.lower():
                    suggestions.extend([
                        "Quer saber mais sobre machine learning?",
                        "Posso explicar sobre deep learning?",
                        "Interessado em aplica√ß√µes pr√°ticas da IA?"
                    ])
                elif "filosofia" in query.lower():
                    suggestions.extend([
                        "Quer explorar outras correntes filos√≥ficas?",
                        "Posso explicar sobre l√≥gica formal?",
                        "Interessado em matem√°tica e filosofia?"
                    ])
                else:
                    suggestions.extend([
                        "Posso ajudar com outros t√≥picos?",
                        "Quer fazer uma pergunta mais espec√≠fica?",
                        "Posso explicar melhor algum conceito?"
                    ])
                
                return suggestions[:3]  # Return top 3 suggestions
            
            def collect_feedback(self, feedback_type, rating, comment=""):
                """Collect user feedback"""
                feedback = {
                    "timestamp": time.time(),
                    "type": feedback_type,
                    "rating": rating,
                    "comment": comment
                }
                self.user_feedback.append(feedback)
                return feedback
        
        # Initialize interaction flow
        interaction_flow = UserInteractionFlow()
        
        # Test interaction scenarios
        test_scenarios = [
            {
                "input": "O que √© intelig√™ncia artificial?",
                "expected_type": "question"
            },
            {
                "input": "Obrigado pela explica√ß√£o!",
                "expected_type": "gratitude"
            },
            {
                "input": "Como posso usar este sistema?",
                "expected_type": "help_request"
            },
            {
                "input": "Acho que entendi o conceito",
                "expected_type": "statement"
            }
        ]
        
        interaction_results = []
        for scenario in test_scenarios:
            try:
                response = interaction_flow.process_user_input(scenario["input"])
                
                # Generate suggestions
                suggestions = interaction_flow.generate_suggestions(
                    scenario["input"], 
                    {"previous_queries": ["test query"]}
                )
                
                interaction_results.append({
                    "input": scenario["input"],
                    "expected_type": scenario["expected_type"],
                    "detected_type": response["type"],
                    "response": response["response"],
                    "suggestions_count": len(suggestions),
                    "suggestions": suggestions,
                    "success": response["type"] == scenario["expected_type"]
                })
                
            except Exception as e:
                interaction_results.append({
                    "input": scenario["input"],
                    "success": False,
                    "error": str(e)
                })
        
        # Test feedback collection
        feedback_tests = [
            {"type": "thumbs_up", "rating": 5, "comment": "Muito √∫til!"},
            {"type": "thumbs_down", "rating": 2, "comment": "N√£o foi claro"},
            {"type": "rating", "rating": 4, "comment": "Bom, mas pode melhorar"}
        ]
        
        for feedback_test in feedback_tests:
            interaction_flow.collect_feedback(
                feedback_test["type"],
                feedback_test["rating"],
                feedback_test["comment"]
            )
        
        # Calculate interaction effectiveness
        successful_interactions = sum(1 for r in interaction_results if r["success"])
        total_interactions = len(interaction_results)
        interaction_effectiveness = successful_interactions / total_interactions if total_interactions > 0 else 0
        
        results = {
            "test_name": "Phase 3.4 User Interaction Flow Test",
            "passed": interaction_effectiveness >= 0.8,
            "interaction_effectiveness": interaction_effectiveness,
            "successful_interactions": successful_interactions,
            "total_interactions": total_interactions,
            "feedback_collected": len(interaction_flow.user_feedback),
            "interaction_results": interaction_results,
            "feedback_data": interaction_flow.user_feedback
        }
        
        print(f"  ‚úÖ Interaction Effectiveness: {interaction_effectiveness:.2%}")
        print(f"  ‚úÖ Successful Interactions: {successful_interactions}/{total_interactions}")
        print(f"  ‚úÖ Feedback Collected: {len(interaction_flow.user_feedback)}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        return {"error": str(e), "passed": False}

def run_phase3_comprehensive_validation():
    """Run comprehensive Phase 3 validation"""
    print("üöÄ Phase 3 Agentic Transformation Validation")
    print("=" * 70)
    print("Testing prompt engineering, memory management, reasoning, and interaction")
    print("=" * 70)
    
    start_time = time.time()
    
    # Run all Phase 3 tests
    prompt_results = test_prompt_engineering_effectiveness()
    memory_results = test_conversation_memory_management()
    reasoning_results = test_agentic_reasoning_validation()
    interaction_results = test_user_interaction_flow()
    
    # Calculate overall Phase 3 score
    prompt_score = 1.0 if prompt_results.get('passed', False) else 0.0
    memory_score = 1.0 if memory_results.get('passed', False) else 0.0
    reasoning_score = 1.0 if reasoning_results.get('passed', False) else 0.0
    interaction_score = 1.0 if interaction_results.get('passed', False) else 0.0
    
    # Weighted scoring
    overall_score = (prompt_score * 0.25 + memory_score * 0.25 + 
                    reasoning_score * 0.25 + interaction_score * 0.25)
    
    end_time = time.time()
    duration = end_time - start_time
    
    print(f"\nüéØ Phase 3 Validation Results")
    print("=" * 50)
    print(f"3.1 Prompt Engineering: {prompt_score:.3f}")
    print(f"3.2 Memory Management: {memory_score:.3f}")
    print(f"3.3 Agentic Reasoning: {reasoning_score:.3f}")
    print(f"3.4 User Interaction: {interaction_score:.3f}")
    print(f"Overall Phase 3 Score: {overall_score:.3f}")
    print(f"Duration: {duration:.2f}s")
    print(f"Status: {'‚úÖ PASS' if overall_score >= 0.8 else '‚ùå FAIL'}")
    
    # Save comprehensive results
    results = {
        "phase": "3",
        "test_name": "Phase 3 Agentic Transformation Validation",
        "overall_score": overall_score,
        "prompt_results": prompt_results,
        "memory_results": memory_results,
        "reasoning_results": reasoning_results,
        "interaction_results": interaction_results,
        "duration": duration,
        "passed": overall_score >= 0.8,
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    with open("phase3_agentic_transformation_validation_results.json", "w") as f:
        json.dump(results, f, indent=2)
    
    return results

if __name__ == "__main__":
    results = run_phase3_comprehensive_validation()
    print(f"\nFinal Phase 3 Result: {'‚úÖ PASS' if results['passed'] else '‚ùå FAIL'}")
